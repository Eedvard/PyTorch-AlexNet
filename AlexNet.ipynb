{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch \n",
    "import requests, zipfile, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils, datasets\n",
    "import random, matplotlib\n",
    "import pandas as pd\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the transformations for the data\n",
    "#For the test data the horizontal flip is not required.\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                        transforms.Resize(227),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))\n",
    "                             ])\n",
    "test_transform = transforms.Compose([transforms.Resize(227),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.FashionMNIST('FMNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "#Split the data for training set and validation set\n",
    "trainset, valset = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=True)\n",
    "\n",
    "#Load the testing data\n",
    "testset = datasets.FashionMNIST('FMNIST_data/', download=True, train=False, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__() \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.fc1   = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2   = nn.Linear(4096, 4096)\n",
    "        self.fc3   = nn.Linear(4096, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found\n"
     ]
    }
   ],
   "source": [
    "#Initialize tensorboard writers to visualize the training process\n",
    "writer = SummaryWriter('graphs/training')\n",
    "val_writer = SummaryWriter('graphs/validation')\n",
    "\n",
    "PATH = './net.pth'\n",
    "net = Network(num_classes=10)\n",
    "\n",
    "loadnet = True #Change this to false if don't want to load the existing trained net\n",
    "\n",
    "if(loadnet):\n",
    "    if os.path.exists(PATH):\n",
    "        net.load_state_dict(torch.load(PATH))\n",
    "        net.eval()\n",
    "        print(\"Checkpoint loaded\")\n",
    "    else:\n",
    "        print(\"No checkpoint found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, loss_function, data_loader):\n",
    "    net.eval()\n",
    "    correct = 0.0\n",
    "    num_images = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        images, labels = sample\n",
    "        outs = net(images)\n",
    "        _, preds = outs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "        running_loss += loss_function(outs, labels).item()\n",
    "        num_images += len(labels)\n",
    "\n",
    "    acc = correct.float() / num_images\n",
    "    loss = running_loss / len(data_loader)\n",
    "    return acc, loss\n",
    "\n",
    "def train(net, train_loader, valid_loader, writer, val_writer, loss_function):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    log_every = 100\n",
    "    epoches = 20\n",
    "    last_val = 0\n",
    "    for epoch in range(epoches):\n",
    "        start_t = time.time()\n",
    "        net.train() \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, sample in enumerate(train_loader):\n",
    "            images, labels = sample\n",
    "            outs = net(images)\n",
    "            loss = loss_function(outs, labels) \n",
    "            _, preds = outs.max(1)\n",
    "            correct = preds.eq(labels).sum()\n",
    "\n",
    "            running_acc += correct.float() / len(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            \n",
    "            if i % log_every == 99: \n",
    "                \n",
    "                print('[Epoch/iter]: [{}/{}], loss: {:.05f}, accuracy: {:.05f}'.format(epoch, i+1, \n",
    "                           running_loss / log_every, running_acc / log_every))\n",
    "\n",
    "                log_index = epoch * len(train_loader) + i\n",
    "                writer.add_scalar('Loss', running_loss / log_every, log_index) \n",
    "                writer.add_scalar('Accuracy', running_acc / log_every, log_index)\n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "\n",
    "        \n",
    "        acc_eval, loss_eval = eval(net, loss_function, valid_loader)\n",
    "        print('Elapsed time: {:.02f} seconds, end of epoch: {}, lr: {}, val_loss: {:.05f}, val_acc: {:.05f}'.format(\n",
    "            time.time()-start_t, epoch, optimizer.param_groups[0]['lr'], loss_eval, acc_eval))\n",
    "        val_writer.add_scalar('Loss', loss_eval, log_index)\n",
    "        val_writer.add_scalar('Accuracy', acc_eval, log_index)\n",
    "        if(loss_eval>last_val and last_val != 0):  #If the validation loss stops improving the learning rate is divided by 10.\n",
    "            last_val = loss_eval\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']/10\n",
    "\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to train the network\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-54e336507357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Beginning to train the network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrained_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-02703079eb84>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, valid_loader, writer, val_writer, loss_function)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ac/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ac/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Beginning to train the network')\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "trained_net = train(net, trainloader, validloader, writer, val_writer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, loss_test = eval(net, loss_function, testloader)\n",
    "print('Accuracy on testing data: {:.05f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
