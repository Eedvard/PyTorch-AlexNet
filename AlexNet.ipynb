{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch \n",
    "import requests, zipfile, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils, datasets\n",
    "import random, matplotlib\n",
    "import pandas as pd\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the transformations for the data\n",
    "#For the test data the horizontal flip is not required.\n",
    "\n",
    "transform = transforms.Compose([.RandomHorizontalFlip(p=0.5),\n",
    "                        transforms.Resize(227),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))\n",
    "                             ])\n",
    "test_transform = transforms.Compose([transforms.Resize(227),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5,), (0.5,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.FashionMNIST('FMNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "#Split the data for training set and validation set\n",
    "trainset, valset = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=True)\n",
    "\n",
    "#Load the testing data\n",
    "testset = datasets.FashionMNIST('FMNIST_data/', download=True, train=False, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__() \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=11, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5, inplace=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.fc1   = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2   = nn.Linear(4096, 4096)\n",
    "        self.fc3   = nn.Linear(4096, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found\n"
     ]
    }
   ],
   "source": [
    "#Initialize tensorboard writers to visualize the training process\n",
    "writer = SummaryWriter('graphs/training')\n",
    "val_writer = SummaryWriter('graphs/validation')\n",
    "\n",
    "PATH = './net.pth'\n",
    "net = Network(num_classes=10)\n",
    "\n",
    "loadnet = True #Change this to false if don't want to load the existing trained net\n",
    "\n",
    "if(loadnet):\n",
    "    if os.path.exists(PATH):\n",
    "        net.load_state_dict(torch.load(PATH))\n",
    "        net.eval()\n",
    "        print(\"Checkpoint loaded\")\n",
    "    else:\n",
    "        print(\"No checkpoint found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, loss_function, data_loader):\n",
    "    net.eval()\n",
    "    correct = 0.0\n",
    "    num_images = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, sample in enumerate(data_loader):\n",
    "        images, labels = sample\n",
    "        outs = net(images)\n",
    "        _, preds = outs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "        running_loss += loss_function(outs, labels).item()\n",
    "        num_images += len(labels)\n",
    "\n",
    "    acc = correct.float() / num_images\n",
    "    loss = running_loss / len(data_loader)\n",
    "    return acc, loss\n",
    "\n",
    "def train(net, train_loader, valid_loader, writer, val_writer, loss_function):\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    log_every = 100\n",
    "    epoches = 20\n",
    "    last_val = 0\n",
    "    for epoch in range(epoches):\n",
    "        start_t = time.time()\n",
    "        net.train() \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        for i, sample in enumerate(train_loader):\n",
    "            images, labels = sample\n",
    "            outs = net(images)\n",
    "            loss = loss_function(outs, labels) \n",
    "            _, preds = outs.max(1)\n",
    "            correct = preds.eq(labels).sum()\n",
    "\n",
    "            running_acc += correct.float() / len(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            \n",
    "            if i % log_every == 99: \n",
    "                \n",
    "                print('[Epoch/iter]: [{}/{}], loss: {:.05f}, accuracy: {:.05f}'.format(epoch, i+1, \n",
    "                           running_loss / log_every, running_acc / log_every))\n",
    "\n",
    "                log_index = epoch * len(train_loader) + i\n",
    "                writer.add_scalar('Loss', running_loss / log_every, log_index) \n",
    "                writer.add_scalar('Accuracy', running_acc / log_every, log_index)\n",
    "                running_loss = 0.0\n",
    "                running_acc = 0.0\n",
    "\n",
    "        \n",
    "        acc_eval, loss_eval = eval(net, loss_function, valid_loader)\n",
    "        print('Elapsed time: {:.02f} seconds, end of epoch: {}, lr: {}, val_loss: {:.05f}, val_acc: {:.05f}'.format(\n",
    "            time.time()-start_t, epoch, optimizer.param_groups[0]['lr'], loss_eval, acc_eval))\n",
    "        val_writer.add_scalar('Loss', loss_eval, log_index)\n",
    "        val_writer.add_scalar('Accuracy', acc_eval, log_index)\n",
    "        if(loss_eval>last_val and last_val != 0):  #If the validation loss stops improving the learning rate is divided by 10.\n",
    "            last_val = loss_eval\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']/10\n",
    "\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to train the network\n",
      "[Epoch/iter]: [0/100], loss: 2.30143, accuracy: 0.11625\n",
      "[Epoch/iter]: [0/200], loss: 2.19989, accuracy: 0.17203\n",
      "[Epoch/iter]: [0/300], loss: 1.09738, accuracy: 0.57563\n",
      "Elapsed time: 1911.91 seconds, end of epoch: 0, lr: 0.005, val_loss: 0.81528, val_acc: 0.71620\n"
     ]
    }
   ],
   "source": [
    "print('Beginning to train the network')\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "trained_net = train(net, trainloader, validloader, writer, val_writer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, loss_test = eval(net, loss_function, testloader)\n",
    "print('Accuracy on testing data: {:.05f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
